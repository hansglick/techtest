{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 37 years old\n",
    "\n",
    "## MY TRAINING\n",
    " * I've always been in the data in some ways\n",
    " * I've been trained in the exploitation of data in three stages\n",
    " * I had my license in 2008\n",
    " * I took evening classes to get my master degree in 2014\n",
    " * I took tons of MOOCs from 2015 to 2016\n",
    "\n",
    "## MY CAREER\n",
    " * Clear line in my career\n",
    " * This demarcation is 2013-2014\n",
    "#### PART I\n",
    " * Before this date, statistician in the healtcare field, on clincial trials and observational studies\n",
    " * Not very invested because of poker (key point)\n",
    "#### PART II\n",
    " * As of this date, I had a desire to become better, to understand and know everything about data science\n",
    " * As a consequence, I took a ton of classes about everything\n",
    "#### WHY THIS SWITCH? 2 REASONS\n",
    "\n",
    " * Until this date, the courses I had taken were all about regression logistics and decision trees\n",
    " * I'd never heard of python before 2014\n",
    " * This gave me a very limited vision of what was data science\n",
    " * Not very intersting\n",
    " \n",
    " +\n",
    " \n",
    " * 2013-2014 coincides with the start of the MOOCs plateform\n",
    " * Which gave me the access of a whole new methods and languages I had never heard before\n",
    " * It made the data science deeper than I used to see it\n",
    " \n",
    " +\n",
    " \n",
    " * I stopped playing poker for some reasons\n",
    "\n",
    "####  2014\n",
    " * Starting in 2014, I've done a series of jobs as a consultant in data science\n",
    " * Until I had the chance to join Bleckwen in 2017\n",
    " \n",
    "#### Bleckwen and Suneris\n",
    " * Bleckwen anti fraud, Suneris military intelligence\n",
    " * Bleckwen and Suneris, the company I'm currently work for, belonged to the same group\n",
    " * In 2017, I'was sort of lent to Suneris so I never work explicitly on anti fraud problems\n",
    " * Even though I worked for Suneris, I was under Bleckwen's supervision\n",
    " * (The two companies' offices were right next door)\n",
    " * Please think of these two experiences on my resume as only one\n",
    "\n",
    "#### Turning point\n",
    " * I thought I was the best (I have done pretty good scores on some Kaggle contests)\n",
    " * It turned out I was not\n",
    " * Surrounded by talented people for the first time\n",
    " * They taught me what it takes to be a complete data scientist\n",
    " \n",
    " 1. Software engineering\n",
    "  * Not only about training model\n",
    "  * About deliver an application within a team working environnement\n",
    "  * How to use git\n",
    "  * How write precise documentation\n",
    "  * How to code properly, how to design an application, build independant blocks, build functions that generalise well\n",
    "  * How to build unit tests\n",
    " \n",
    " 2. Communication\n",
    "  * How to make sure you understand the business question\n",
    "  * How to convince the decision-makers of your algorithm\n",
    "  * Quickly tell the team the limitations of your algorithm\n",
    "  * Organize daily meetings to ensure we stay on track\n",
    " \n",
    " 3. Know my strength\n",
    "  * As I mentionned before, surrounded by talented people, better than me on many aspects\n",
    "  * Saw that I play in the same league as the best when it comes to develop algorithm to solve non trivial problems\n",
    "  * It boosted my confidence\n",
    "  * How to frame a problem, turn a vague business question into a series of data science question and build an application from them\n",
    "\n",
    "\n",
    "#### APPS\n",
    " * Habits extraction\n",
    " * Breakoff habit\n",
    " * Twitter events extraction\n",
    " * Neighborhood’s evolution\n",
    " * Plumber problem\n",
    " * Convoys detection\n",
    " * Meeting algorithm\n",
    " * Destination prediction\n",
    " * Face recognition\n",
    " * Authorship signature\n",
    " * Deanonymize coordinates\n",
    "\n",
    "\n",
    "#### Prez apps\n",
    " * I've developed a lot of applications that involve\n",
    " * Mostly unsupervised models, sometimes supervised\n",
    " * Images data, text data\n",
    " * Mostly involve graph theory\n",
    " * Optimization problem\n",
    " * A lot lrelated to spatio temporal data\n",
    "I propose you to choose among the most complex ones: clothes extraction, events extraction from twitter, authorship signature, face recognition, neighborhood evolution, etc.\n",
    "\n",
    "\n",
    "\n",
    "#### Habits\n",
    " * Extract habit of a target based on his communications (spatio temporal data)\n",
    " * **Spatial block**\n",
    "  * Hierarchical tree that represent hierarchical spatial habit\n",
    "  * Based on sequence of ddbscans with decreasing epsilon\n",
    "  * Live in newbridge and work in Dublin\n",
    "  * The first node represent dublin\n",
    "  * Then underneath the dublin node you may find two nodes which represent newbridge and dublin\n",
    "  * Then underneath the dublin node you may find two nodes that represent your work location and your favorite restaurant\n",
    "  * That way, you can represent complex spatial habits\n",
    " * **Temporal block**\n",
    "  * Hierarchical tree that represent temporal habits of a target within a spatial polygon which is Actually a node from the spatial tree\n",
    "  * Each level represent a level of temporal recurrence, ex : weeks, days, hours\n",
    "  * For each level you may find one or multiple nodes.\n",
    "  * Each node represent a temporal reccurence, for exemple every two weeks, every Sunday\n",
    "  * Then at the bottom you find the final nodes.\n",
    "  * Each final node is associated with 2 statistics : the precision and the raw numbers\n",
    "  * Let’s say you go to the restaurant every two weeks on Friday\n",
    "  * The top node represent a biweek pattern\n",
    "  * Then underneath, linked to that node, the Friday node\n",
    "  * Then underneath, linked to that node, a 12 to 2 pm pattern node\n",
    "  * The tree is built in order to maximize a metric because sure, we want the maximum precision and the minimum confidence intervals\n",
    " \n",
    "  * **Concretely**\n",
    "  * We generate a tree at each step, that represent your habits for a particular location\n",
    "  * a smarter version of genetic algorithm\n",
    "  * Saturday and Sunday, 67% of the time 8/12\n",
    "  * Saturday, 83% of the time, 5/6\n",
    "  * Simulate bunch of periods for each tree\n",
    "  * A cost is associated for each case possible, True Positive, True Negative, and so on\n",
    "  * You can compute a cost for each tree\n",
    "\n",
    "#### Events extraction\n",
    " * Extract events from the meta data of the communications of a population\n",
    " * Collect data about catalan crisis during a particular period\n",
    " * Keep the most retweeted tweets only\n",
    " * A tweet is a reaction to an event (a comment)\n",
    " * 2 tweets refers to the same event if\n",
    " * Got retweeted in the same time window\n",
    " * Got retweeted by the same people\n",
    " * If lotta people retweet A and B in the same period then A and B refers prolly to the same event\n",
    " * Define a metric that takes in account both of the conditions\n",
    " * Graph of tweets + clustering\n",
    " * Cluster is an abstraction of an event\n",
    " * Turned out that a tweet is more than just a reaction, it is a reaction through a political opinion\n",
    " * Get a political map of a particular topic\n",
    " * Who belong to which topic\n",
    " * What kind of event make this guy react\n",
    " * Events could be demonstrations, an article pusblished in a newspaper, a debate on tv, a video pusblished on youtube\n",
    "\n",
    "\n",
    "#### Authorship signature\n",
    " * Get information about the author according to the documents he has written\n",
    " * Collection of documents\n",
    " * Our main problem was that the languages used in our clients' countries are very uncommon languages for which there are no pre-trained embeddings.\n",
    " * Build embeddings of words\n",
    " * Clustering of the population based on social graph\n",
    " * Clusters might represent criminal families, ethnicities, spatial regions, social levels\n",
    " * Train a deep learning network that takes a document as input and predict the cluster the document belongs to\n",
    " * That way if you have documents from a guy you do not know, you can take a guess about the family he belongs\n",
    " * **Concretely**\n",
    " * Each sentence is mapped to a cluster for a particular level\n",
    " * Each document is mapped to a cluster by majority vote of their predicted sentences\n",
    " * For each level, Each individual is associated with a distribution of clusters, representing the frequency of documents belonging to a particular group\n",
    " * That way you can capture subtelties, like the heterogeneity of the clusters he belonged to\n",
    " * Let's imagine your distribution is 50% english cluster, 30% russian, 20% hebrew\n",
    " * No labels, we pick up representatives documents to define the clusters\n",
    " * We choose the most caricatural representatives to define the clusters\n",
    " * To be more precise, at the application level, two things are displayed:\n",
    " * The hierarchical tree of clusters and the sentences with the highest scores in each cluster so that the analyst can have an idea of the label of each cluster\n",
    "\n",
    "#### Face recognition\n",
    " * Flow of faces\n",
    " * Database of faces\n",
    " * Goal : map each new face to an existing entry in the database or create a new one\n",
    " * Pretrained network we use does not contain enough faces from the appropriate region of the world we want\n",
    " * Retrained a model called facenet, where the triplet loss is involved\n",
    "\n",
    "\n",
    "#### Taxi Trip\n",
    " * Inputs were the starting and ending point and the time departure\n",
    " * Predict the travel time\n",
    " * Travel in a city is a path in a road network\n",
    " * Basic shortest path algorithm was not efficient here because road network has an enormous diameter contrary to the basic social network everybody see when it comes to learn graph theory, famous 5 steps to reach anybody in the world\n",
    "\n",
    "\n",
    "#### Neighborhood’s evolution\n",
    " * Track target neighborhood over time\n",
    " * Neighborhood :\n",
    " * The people with who he interacts in some way\n",
    " * We raise an alert if the speed of incursion of a neighbor is too high\n",
    " * Speed of incursion could be negative\n",
    " * If I divorce my wife, which I hope I will not do, my wife is going to have this negative speed of incursion, she will disapear from my neighborhood\n",
    "\n",
    "\n",
    "\n",
    "#### QUESTIONS\n",
    " * One of your lecture on the relationship between the beauty of the environment in which we live and happiness\n",
    " * Scenic beauty is negatively correlated with population density\n",
    " * In any case, I haven't seen big scores for crowded landscapes\n",
    " * Does that mean that in conclusion one can say that human being needs calm to be happy\n",
    " \n",
    "\n",
    "#### TRICKS\n",
    " * You do well to tell me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
